{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** Cristina Caro Gonz√°lez\n"
      ],
      "metadata": {
        "id": "I-zvxHdW57X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:**\n",
        "\n",
        "Develop and train a Feedforward Artificial Neural Network in Keras in the following classification problem. The objective class is called \"y\" and it's the first column in both csv.\n",
        "\n",
        "If you see the dataset is very big and you machine is not powerful enough to train in a reasonable time using it you may pick a subset as we saw in the notebook.\n",
        "\n",
        "You may upload either a .py or a Jupyter Notebook."
      ],
      "metadata": {
        "id": "Rx91vjUU6aA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to try several methods, and little by little improving the methods, introducing regularization, dropout, etc. Methods that we have seen in clase, to improve our deep learning models."
      ],
      "metadata": {
        "id": "zV3XB-Ms6q7e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lfU2Q7ME2LA",
        "outputId": "399df332-9835-47d3-cadf-c9d234d34d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 8s 4ms/step - loss: 1.9042 - accuracy: 0.7922 - val_loss: 0.5272 - val_accuracy: 0.8684\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.4125 - accuracy: 0.8957 - val_loss: 0.3583 - val_accuracy: 0.9119\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2999 - accuracy: 0.9220 - val_loss: 0.2877 - val_accuracy: 0.9294\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2305 - accuracy: 0.9388 - val_loss: 0.2715 - val_accuracy: 0.9273\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2019 - accuracy: 0.9454 - val_loss: 0.2249 - val_accuracy: 0.9402\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1791 - accuracy: 0.9499 - val_loss: 0.1888 - val_accuracy: 0.9512\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1543 - accuracy: 0.9570 - val_loss: 0.1877 - val_accuracy: 0.9513\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1364 - accuracy: 0.9618 - val_loss: 0.1783 - val_accuracy: 0.9565\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1260 - accuracy: 0.9644 - val_loss: 0.1571 - val_accuracy: 0.9581\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1147 - accuracy: 0.9671 - val_loss: 0.1910 - val_accuracy: 0.9547\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9569\n",
            "Test Loss: 0.1903935819864273\n",
            "Test Accuracy: 0.9569000005722046\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the train and test dataset\n",
        "train_df = pd.read_csv(\"w4_train_data.csv\") ## We read the training dataset of our exercise\n",
        "test_df = pd.read_csv(\"w4_test_data.csv\") ## We read the testing dataset of our exercise\n",
        "\n",
        "# test_df is the test dataset that is used to evaluate the final performance of the model. It contains the input features and the target(s) variables. The test_df is used \n",
        "# to measure the accuracy of the model after it has been trained on the training dataset.\n",
        "\n",
        "# Split the train dataset into input features (X) and target variable (y)\n",
        "X = train_df.iloc[:, 1:] # all columns except the first one, because the first one is our target variable.\n",
        "y = train_df.iloc[:, 0] # first column, i.e, our target variable.\n",
        "\n",
        "# Convert the target variable to categorical\n",
        "y = to_categorical(y, num_classes=10) # We have 10 categories, because the target value goes from 0 to 9, i.e. 10 different values.\n",
        "\n",
        "# Split the train data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # In deep learning is very important to use a training and validation set. After training our model here, we will test it in the test dataset.\n",
        "# I select a validation size of 20% . We use a seed of 42 so we can replicate the results.\n",
        "\n",
        "# X_val and y_val are the validation data, a subset of the training data that is used to tune the hyperparameters of the model. The validation data is used to \n",
        "# monitor the performance of the model during training and to prevent overfitting, which, as we saw in the classes, is very common problem in deep learning\n",
        "\n",
        "# Define the model architecture\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # I select the loss funcion categorical crossentropy, optimizer adam and metric accuracy.\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val)) # To start, we create a basic model with only 10 epochs\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(test_df.iloc[:, 1:], to_categorical(test_df.iloc[:, 0], num_classes=10)) # We test the results of the first model\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "\n",
        "# We can see that the final results are not good, this means we will have to improve the model using other techniques as regularization, dropout and number of epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this first model, the results could be better. As we learnt in class, we can use different techniques to improve our model.\n",
        "Some of them are: increase the number of epochs, use regularization techniques, increase the size of the model adding more layers or increasing the number of neurons, trying with different activation functions and use normalization and standarization, very important in deep learning.\n",
        "\n",
        "In this second model, we are going to use regularization L2.\n",
        "\n",
        "Additionally, we increase the number of epochs, from 10 to 40.\n",
        "\n",
        "The rest of the model is basically the same as the previous one.\n",
        "\n",
        "In this case, I avoid to repeat the comments I put in the previous model, as the would be the same."
      ],
      "metadata": {
        "id": "85wZlQpQ9s8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.regularizers import l2 # This is to apply the regularization L2\n",
        "\n",
        "# Load the train and test datasets\n",
        "train_df = pd.read_csv(\"w4_train_data.csv\")\n",
        "test_df = pd.read_csv(\"w4_test_data.csv\")\n",
        "\n",
        "# Split the train dataset into input features (X) and target variable (y)\n",
        "X = train_df.iloc[:, 1:] # all columns except the first one\n",
        "y = train_df.iloc[:, 0] # first column\n",
        "\n",
        "# Convert the target variable to categorical\n",
        "y = to_categorical(y, num_classes=10)\n",
        "\n",
        "# Split the train data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(784,), kernel_regularizer=l2(0.001)))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# We have to be careful with the regularization parameter, the model can get worse results if we use a L2 regularization with a weight too large, and it will underfit the training data.\n",
        "# To improve the performance, we should reduce the L2 regularization weight. In this case, I have chosen 0.001\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(test_df.iloc[:, 1:], to_categorical(test_df.iloc[:, 0], num_classes=10))\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tLkVPVVHBpI",
        "outputId": "c0c3ec7f-a7ce-4d02-eb37-aae5db9c962b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 2.0858 - accuracy: 0.5950 - val_loss: 0.8436 - val_accuracy: 0.7705\n",
            "Epoch 2/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.7221 - accuracy: 0.8206 - val_loss: 0.5681 - val_accuracy: 0.8852\n",
            "Epoch 3/40\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.4643 - accuracy: 0.9019 - val_loss: 0.4114 - val_accuracy: 0.9218\n",
            "Epoch 4/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3432 - accuracy: 0.9312 - val_loss: 0.3161 - val_accuracy: 0.9406\n",
            "Epoch 5/40\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2879 - accuracy: 0.9441 - val_loss: 0.2940 - val_accuracy: 0.9442\n",
            "Epoch 6/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2494 - accuracy: 0.9507 - val_loss: 0.2637 - val_accuracy: 0.9498\n",
            "Epoch 7/40\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2286 - accuracy: 0.9544 - val_loss: 0.2407 - val_accuracy: 0.9493\n",
            "Epoch 8/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2131 - accuracy: 0.9585 - val_loss: 0.2503 - val_accuracy: 0.9518\n",
            "Epoch 9/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2024 - accuracy: 0.9596 - val_loss: 0.2236 - val_accuracy: 0.9572\n",
            "Epoch 10/40\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1993 - accuracy: 0.9609 - val_loss: 0.2372 - val_accuracy: 0.9547\n",
            "Epoch 11/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1887 - accuracy: 0.9640 - val_loss: 0.2204 - val_accuracy: 0.9579\n",
            "Epoch 12/40\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1861 - accuracy: 0.9644 - val_loss: 0.2650 - val_accuracy: 0.9469\n",
            "Epoch 13/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1842 - accuracy: 0.9645 - val_loss: 0.2264 - val_accuracy: 0.9546\n",
            "Epoch 14/40\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1826 - accuracy: 0.9659 - val_loss: 0.2346 - val_accuracy: 0.9542\n",
            "Epoch 15/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1783 - accuracy: 0.9669 - val_loss: 0.2206 - val_accuracy: 0.9551\n",
            "Epoch 16/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1723 - accuracy: 0.9692 - val_loss: 0.2090 - val_accuracy: 0.9578\n",
            "Epoch 17/40\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1744 - accuracy: 0.9671 - val_loss: 0.2010 - val_accuracy: 0.9582\n",
            "Epoch 18/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1693 - accuracy: 0.9686 - val_loss: 0.2277 - val_accuracy: 0.9542\n",
            "Epoch 19/40\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1704 - accuracy: 0.9687 - val_loss: 0.2089 - val_accuracy: 0.9602\n",
            "Epoch 20/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1661 - accuracy: 0.9691 - val_loss: 0.2149 - val_accuracy: 0.9603\n",
            "Epoch 21/40\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1701 - accuracy: 0.9690 - val_loss: 0.2097 - val_accuracy: 0.9610\n",
            "Epoch 22/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1605 - accuracy: 0.9719 - val_loss: 0.2316 - val_accuracy: 0.9554\n",
            "Epoch 23/40\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1683 - accuracy: 0.9689 - val_loss: 0.2113 - val_accuracy: 0.9587\n",
            "Epoch 24/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1622 - accuracy: 0.9716 - val_loss: 0.2307 - val_accuracy: 0.9558\n",
            "Epoch 25/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1667 - accuracy: 0.9701 - val_loss: 0.2416 - val_accuracy: 0.9504\n",
            "Epoch 26/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1631 - accuracy: 0.9711 - val_loss: 0.1991 - val_accuracy: 0.9648\n",
            "Epoch 27/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1630 - accuracy: 0.9706 - val_loss: 0.2151 - val_accuracy: 0.9612\n",
            "Epoch 28/40\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1691 - accuracy: 0.9693 - val_loss: 0.2017 - val_accuracy: 0.9620\n",
            "Epoch 29/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1582 - accuracy: 0.9726 - val_loss: 0.2092 - val_accuracy: 0.9599\n",
            "Epoch 30/40\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1611 - accuracy: 0.9707 - val_loss: 0.2112 - val_accuracy: 0.9633\n",
            "Epoch 31/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1608 - accuracy: 0.9713 - val_loss: 0.2201 - val_accuracy: 0.9608\n",
            "Epoch 32/40\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1610 - accuracy: 0.9718 - val_loss: 0.2159 - val_accuracy: 0.9592\n",
            "Epoch 33/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1602 - accuracy: 0.9709 - val_loss: 0.2235 - val_accuracy: 0.9592\n",
            "Epoch 34/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1616 - accuracy: 0.9725 - val_loss: 0.2071 - val_accuracy: 0.9615\n",
            "Epoch 35/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1611 - accuracy: 0.9715 - val_loss: 0.2230 - val_accuracy: 0.9571\n",
            "Epoch 36/40\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1590 - accuracy: 0.9723 - val_loss: 0.2288 - val_accuracy: 0.9561\n",
            "Epoch 37/40\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1591 - accuracy: 0.9722 - val_loss: 0.2056 - val_accuracy: 0.9607\n",
            "Epoch 38/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1608 - accuracy: 0.9721 - val_loss: 0.2175 - val_accuracy: 0.9586\n",
            "Epoch 39/40\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1618 - accuracy: 0.9719 - val_loss: 0.2187 - val_accuracy: 0.9597\n",
            "Epoch 40/40\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1585 - accuracy: 0.9727 - val_loss: 0.2043 - val_accuracy: 0.9599\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2081 - accuracy: 0.9602\n",
            "Test Loss: 0.20809772610664368\n",
            "Test Accuracy: 0.9602000117301941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our results, in the second model, have improve a little bit we the changes we made.\n",
        "Let's try a third model with more changes, in this case, we are going to increase the number of epochs to 50, we are going to also apply L2 regularization with a parameter of 0.001, we are going to increase the number of layers, and the number of neurons."
      ],
      "metadata": {
        "id": "8bCr02Z5CCgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following model, we increase the number of neurons, and we add a new layer. We increase the number of epochs to 50 and the batch size from 32 to 64. The batch size determines the number of samples that are used to update the weights of the network in one iteration."
      ],
      "metadata": {
        "id": "2nFgy0rg_8mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Load the train and test datasets\n",
        "train_df = pd.read_csv(\"w4_train_data.csv\")\n",
        "test_df = pd.read_csv(\"w4_test_data.csv\")\n",
        "\n",
        "# Split the train dataset into input features (X) and target variable (y)\n",
        "X = train_df.iloc[:, 1:] # all columns except the first one\n",
        "y = train_df.iloc[:, 0] # first column\n",
        "\n",
        "# Convert the target variable to categorical\n",
        "y = to_categorical(y, num_classes=10)\n",
        "\n",
        "# Split the train data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,), kernel_regularizer=l2(0.001))) #We use regularization as well, and we use 128 neurons in this case.\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val)) #We set the number of epochs to 50 and the batch size to 64\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(test_df.iloc[:, 1:], to_categorical(test_df.iloc[:, 0], 10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzwhNVQvHB04",
        "outputId": "e90fcc5b-4d90-4b11-9db4-04a9eeabaf85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 2.0787 - accuracy: 0.6859 - val_loss: 0.8046 - val_accuracy: 0.8504\n",
            "Epoch 2/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.6596 - accuracy: 0.8956 - val_loss: 0.5396 - val_accuracy: 0.9237\n",
            "Epoch 3/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4776 - accuracy: 0.9358 - val_loss: 0.4453 - val_accuracy: 0.9415\n",
            "Epoch 4/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3861 - accuracy: 0.9516 - val_loss: 0.4003 - val_accuracy: 0.9517\n",
            "Epoch 5/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3332 - accuracy: 0.9597 - val_loss: 0.3789 - val_accuracy: 0.9456\n",
            "Epoch 6/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2944 - accuracy: 0.9622 - val_loss: 0.3243 - val_accuracy: 0.9534\n",
            "Epoch 7/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2679 - accuracy: 0.9657 - val_loss: 0.3025 - val_accuracy: 0.9567\n",
            "Epoch 8/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2364 - accuracy: 0.9696 - val_loss: 0.2640 - val_accuracy: 0.9629\n",
            "Epoch 9/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2130 - accuracy: 0.9712 - val_loss: 0.2312 - val_accuracy: 0.9663\n",
            "Epoch 10/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1956 - accuracy: 0.9736 - val_loss: 0.2339 - val_accuracy: 0.9613\n",
            "Epoch 11/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1836 - accuracy: 0.9747 - val_loss: 0.2265 - val_accuracy: 0.9628\n",
            "Epoch 12/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1701 - accuracy: 0.9757 - val_loss: 0.2093 - val_accuracy: 0.9687\n",
            "Epoch 13/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1645 - accuracy: 0.9764 - val_loss: 0.2066 - val_accuracy: 0.9662\n",
            "Epoch 14/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1560 - accuracy: 0.9777 - val_loss: 0.1989 - val_accuracy: 0.9681\n",
            "Epoch 15/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1544 - accuracy: 0.9776 - val_loss: 0.1837 - val_accuracy: 0.9713\n",
            "Epoch 16/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1511 - accuracy: 0.9778 - val_loss: 0.1945 - val_accuracy: 0.9677\n",
            "Epoch 17/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1473 - accuracy: 0.9783 - val_loss: 0.1836 - val_accuracy: 0.9720\n",
            "Epoch 18/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1411 - accuracy: 0.9802 - val_loss: 0.1883 - val_accuracy: 0.9691\n",
            "Epoch 19/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1388 - accuracy: 0.9794 - val_loss: 0.1922 - val_accuracy: 0.9668\n",
            "Epoch 20/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1411 - accuracy: 0.9791 - val_loss: 0.1919 - val_accuracy: 0.9679\n",
            "Epoch 21/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1368 - accuracy: 0.9804 - val_loss: 0.1815 - val_accuracy: 0.9710\n",
            "Epoch 22/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1370 - accuracy: 0.9799 - val_loss: 0.1840 - val_accuracy: 0.9698\n",
            "Epoch 23/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1322 - accuracy: 0.9813 - val_loss: 0.1843 - val_accuracy: 0.9686\n",
            "Epoch 24/50\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.1326 - accuracy: 0.9809 - val_loss: 0.1784 - val_accuracy: 0.9719\n",
            "Epoch 25/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1315 - accuracy: 0.9811 - val_loss: 0.1877 - val_accuracy: 0.9667\n",
            "Epoch 26/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1319 - accuracy: 0.9806 - val_loss: 0.1745 - val_accuracy: 0.9718\n",
            "Epoch 27/50\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.1307 - accuracy: 0.9815 - val_loss: 0.1758 - val_accuracy: 0.9728\n",
            "Epoch 28/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1306 - accuracy: 0.9805 - val_loss: 0.1862 - val_accuracy: 0.9659\n",
            "Epoch 29/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1258 - accuracy: 0.9822 - val_loss: 0.1829 - val_accuracy: 0.9683\n",
            "Epoch 30/50\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.1277 - accuracy: 0.9820 - val_loss: 0.1729 - val_accuracy: 0.9721\n",
            "Epoch 31/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1261 - accuracy: 0.9821 - val_loss: 0.1954 - val_accuracy: 0.9654\n",
            "Epoch 32/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1256 - accuracy: 0.9817 - val_loss: 0.1954 - val_accuracy: 0.9638\n",
            "Epoch 33/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1257 - accuracy: 0.9825 - val_loss: 0.1706 - val_accuracy: 0.9732\n",
            "Epoch 34/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1212 - accuracy: 0.9829 - val_loss: 0.1711 - val_accuracy: 0.9698\n",
            "Epoch 35/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1239 - accuracy: 0.9819 - val_loss: 0.1740 - val_accuracy: 0.9725\n",
            "Epoch 36/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1247 - accuracy: 0.9824 - val_loss: 0.1681 - val_accuracy: 0.9718\n",
            "Epoch 37/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1215 - accuracy: 0.9827 - val_loss: 0.1887 - val_accuracy: 0.9663\n",
            "Epoch 38/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1184 - accuracy: 0.9835 - val_loss: 0.1647 - val_accuracy: 0.9732\n",
            "Epoch 39/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1208 - accuracy: 0.9827 - val_loss: 0.1755 - val_accuracy: 0.9691\n",
            "Epoch 40/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1182 - accuracy: 0.9837 - val_loss: 0.1779 - val_accuracy: 0.9688\n",
            "Epoch 41/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1185 - accuracy: 0.9831 - val_loss: 0.1655 - val_accuracy: 0.9731\n",
            "Epoch 42/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1203 - accuracy: 0.9830 - val_loss: 0.1674 - val_accuracy: 0.9719\n",
            "Epoch 43/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1179 - accuracy: 0.9826 - val_loss: 0.1640 - val_accuracy: 0.9728\n",
            "Epoch 44/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1194 - accuracy: 0.9826 - val_loss: 0.1655 - val_accuracy: 0.9720\n",
            "Epoch 45/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1173 - accuracy: 0.9831 - val_loss: 0.1631 - val_accuracy: 0.9732\n",
            "Epoch 46/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1179 - accuracy: 0.9837 - val_loss: 0.1688 - val_accuracy: 0.9713\n",
            "Epoch 47/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1139 - accuracy: 0.9844 - val_loss: 0.1801 - val_accuracy: 0.9670\n",
            "Epoch 48/50\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.1191 - accuracy: 0.9829 - val_loss: 0.1608 - val_accuracy: 0.9753\n",
            "Epoch 49/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1150 - accuracy: 0.9839 - val_loss: 0.1790 - val_accuracy: 0.9698\n",
            "Epoch 50/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1182 - accuracy: 0.9829 - val_loss: 0.1737 - val_accuracy: 0.9711\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1654 - accuracy: 0.9712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "# We see that in this last model, the results have improved even more."
      ],
      "metadata": {
        "id": "Qv3ULcpCHB6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ce5903-2b8b-4aad-dfee-c434af3a6405"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1654212474822998\n",
            "Test Accuracy: 0.9711999893188477\n"
          ]
        }
      ]
    }
  ]
}